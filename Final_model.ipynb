{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74597ac",
   "metadata": {},
   "source": [
    "### Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad159f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.transform import resize\n",
    "import zarr\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d9959",
   "metadata": {},
   "source": [
    "### Preprocessing functions for SAR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilateral_denoise(image):\n",
    "    return cv2.bilateralFilter(image.astype(np.float32), d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "def apply_clahe(image):\n",
    "    norm = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(norm)\n",
    "\n",
    "def sobel_edges(image):\n",
    "    grad_x = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    magnitude = cv2.magnitude(grad_x, grad_y)\n",
    "    return magnitude\n",
    "\n",
    "def downsample(image, target_shape=(64, 64)):\n",
    "    return resize(image, target_shape, mode='reflect', preserve_range=True, anti_aliasing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d59f00b",
   "metadata": {},
   "source": [
    "### Dataset for Preprocessed SAR Cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad149e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRSARDataset(Dataset):\n",
    "    def __init__(self, root_dir, folder_limit=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = []\n",
    "        zarr_folders = sorted(os.listdir(root_dir))[:folder_limit]\n",
    "\n",
    "        for folder in zarr_folders:\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if not folder_path.endswith(\".zarr\"):\n",
    "                continue\n",
    "            z = zarr.open(folder_path, mode='r')\n",
    "            bands = z['bands']\n",
    "            for i in range(bands.shape[0]):\n",
    "                self.samples.append((folder_path, i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        zarr_path, sample_idx = self.samples[idx]\n",
    "        z = zarr.open(zarr_path, mode='r')\n",
    "        sar_cube = z['bands'][sample_idx]  # [T=4, 2, H, W]\n",
    "\n",
    "        processed = []\n",
    "        for t in range(sar_cube.shape[0]):\n",
    "            vv = sar_cube[t, 0]\n",
    "            vh = sar_cube[t, 1]\n",
    "\n",
    "            vv_denoised = bilateral_denoise(vv)\n",
    "            vh_denoised = bilateral_denoise(vh)\n",
    "            vv_clahe = apply_clahe(vv_denoised)\n",
    "            edge_map = sobel_edges(vv_clahe)\n",
    "\n",
    "            processed.append(np.stack([vv_denoised, vh_denoised, vv_clahe, edge_map], axis=0))\n",
    "\n",
    "        processed = np.stack(processed)  # [T=4, 4, H, W]\n",
    "        mean_img = np.mean(processed, axis=0)\n",
    "        std_img = np.std(processed, axis=0)\n",
    "        diff_img = processed[-1] - processed[0]\n",
    "\n",
    "        temporal_stack = np.concatenate([mean_img, std_img, diff_img], axis=0)\n",
    "        downsampled = np.stack([downsample(temporal_stack[c]) for c in range(temporal_stack.shape[0])])\n",
    "\n",
    "        return torch.tensor(downsampled, dtype=torch.float32), torch.tensor(downsampled, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d087fd",
   "metadata": {},
   "source": [
    "### CNN + Transformer-Based Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=12, compressed_dim=1024):\n",
    "        super(CNNTransformerEncoder, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512), nn.ReLU(),\n",
    "        )\n",
    "        self.proj = nn.Linear(512, 128)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=8, dim_feedforward=256, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.fc = nn.Linear(128, compressed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.view(B, C, H * W).permute(0, 2, 1)\n",
    "        x = self.proj(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x.mean(dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115b12d",
   "metadata": {},
   "source": [
    "### NT-Xent Contrastive Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f71f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
    "    batch_size = z_i.size(0)\n",
    "    z = torch.cat([z_i, z_j], dim=0)\n",
    "    z = F.normalize(z, dim=1)\n",
    "    similarity = torch.mm(z, z.T)\n",
    "    sim_ij = torch.diag(similarity, batch_size)\n",
    "    sim_ji = torch.diag(similarity, -batch_size)\n",
    "    positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "\n",
    "    mask = (~torch.eye(2 * batch_size, dtype=bool)).to(z.device)\n",
    "    negatives = similarity[mask].view(2 * batch_size, -1)\n",
    "\n",
    "    logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n",
    "    labels = torch.zeros(2 * batch_size, dtype=torch.long).to(z.device)\n",
    "    logits /= temperature\n",
    "\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cbb5d5",
   "metadata": {},
   "source": [
    "### SimCLR Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simclr(model, dataset, epochs=10, batch_size=64, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for x1, x2 in pbar:\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "            z1 = model(x1)\n",
    "            z2 = model(x2)\n",
    "            loss = nt_xent_loss(z1, z2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({\"loss\": total_loss / (pbar.n + 1)})\n",
    "\n",
    "    torch.save(model.state_dict(), \"simclr_encoder.pth\")\n",
    "    print(\"âœ… Model saved to simclr_encoder.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab753e",
   "metadata": {},
   "source": [
    "### Extract Embeddings to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, dataset, output_csv=\"sar_embeddings.csv\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "    all_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in tqdm(loader, desc=\"Extracting Embeddings\"):\n",
    "            x = x.to(device)\n",
    "            embeddings = model(x).cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "    all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    df = pd.DataFrame(all_embeddings)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"ðŸ“„ Embeddings saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ea9b9",
   "metadata": {},
   "source": [
    "### Main Script: Training + Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f502e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "    base_path = \"D:/IVP _ project/data/SSL4EO-S12-v1.1/train/S1GRD\"\n",
    "\n",
    "    print(\"ðŸš€ Training on 100 folders\")\n",
    "    train_dataset = SimCLRSARDataset(base_path, folder_limit=100)\n",
    "    model = CNNTransformerEncoder()\n",
    "    train_simclr(model, train_dataset)\n",
    "\n",
    "    print(\"ðŸ“¥ Extracting embeddings from 398 folders\")\n",
    "    full_dataset = SimCLRSARDataset(base_path, folder_limit=398)\n",
    "    model.load_state_dict(torch.load(\"simclr_encoder.pth\"))\n",
    "    extract_embeddings(model, full_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
